{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lesson 4: Multi-Head Attention and Transformer Block\n",
    "\n",
    "Welcome to Lesson 4! We’re taking a big step forward by upgrading from a single attention head (Lesson 3) to **multi-head attention** and building a full **transformer block**, the backbone of models like Llama 2. Don’t worry if this sounds complex—we’ll break it down into simple pieces with lots of examples and visuals. Since you’re new to LLMs, we’ll focus on understanding *why* these ideas matter and *how* they work, using PyTorch and our Chinese text from `data.csv`.\n",
    "\n",
    "## What You’ll Learn\n",
    "- Why **multi-head attention** is better than single-head attention.\n",
    "- How to code multi-head attention with multiple attention heads running in parallel.\n",
    "- What a **transformer block** is and how it combines attention with other layers.\n",
    "- How to visualize attention patterns across multiple heads using our Chinese data.\n",
    "\n",
    "## Our Data\n",
    "We’re still using `data.csv` with the `head` column, featuring casual Hong Kong-style Chinese text (e.g., from Lihkg). Our example sentence is: *法國紅酒慢煮阿根廷牛舌 配 煙肉洋蔥炒著仔* (French red wine slow-cooked Argentine beef tongue paired with bacon onion stir-fry). We’ll see how multiple heads focus on different parts of this sentence!\n",
    "\n",
    "## Prerequisites\n",
    "- PyTorch installed (from Lesson 0).\n",
    "- `data.csv` with columns `user`, `title`, `head`.\n",
    "- Basic Python knowledge and familiarity with Lesson 3’s single-head attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import Libraries\n",
    "\n",
    "Let’s load the tools we need. These are familiar from Lesson 3, with some extras for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed for consistent results\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check PyTorch version\n",
    "print(\"PyTorch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Recap Single-Head Attention\n",
    "\n",
    "In Lesson 3, we built a single attention head. It took our input (e.g., `法國`, `紅酒`, `牛舌`), turned each token into **query (Q)**, **key (K)**, and **value (V)** vectors, and computed:\n",
    "- **Scores**: How much each token relates to others (e.g., `牛舌` focusing on `慢煮`).\n",
    "- **Weights**: Probabilities from softmax (e.g., 0.45 for `慢煮`).\n",
    "- **Output**: A new representation mixing relevant info.\n",
    "\n",
    "But a single head can only focus on *one pattern* at a time. What if `牛舌` needs to focus on *both* its cooking method (`慢煮`) *and* its origin (`阿根廷`)? That’s where **multi-head attention** comes in—it lets the model look at multiple relationships simultaneously!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Understand Multi-Head Attention\n",
    "\n",
    "### Why Multi-Head?\n",
    "Imagine you’re reading our sentence: *法國紅酒慢煮阿根廷牛舌 配 煙肉洋蔥炒著仔*. To understand `牛舌` (beef tongue):\n",
    "- One part of your brain might focus on **how it’s cooked** (`慢煮`, `紅酒`).\n",
    "- Another part might focus on **where it’s from** (`阿根廷`).\n",
    "- A third part might check **what it’s paired with** (`配`, `煙肉`).\n",
    "\n",
    "A single attention head can only pick *one* of these patterns. Multi-head attention runs several heads in parallel (e.g., 4 heads), each learning a different focus, then combines their insights. This makes the model smarter and more flexible!\n",
    "\n",
    "### How It Works\n",
    "1. **Split Into Heads**: Instead of one Q, K, V set, we create multiple sets (e.g., 4 heads/ 4Q,4K,4V).\n",
    "2. **Run Attention Separately**: Each head computes its own attention weights and output.\n",
    "3. **Combine Results**: Concatenate the outputs from all heads and transform them back to the original size.\n",
    "\n",
    "Let’s build it step-by-step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Simulate Input Data\n",
    "\n",
    "Like Lesson 3, we’ll simulate tokenized embeddings for 6 tokens from our sentence: `法國`, `紅酒`, `慢煮`, `阿根廷`, `牛舌`, `配`. In a real model, these come from a tokenizer and embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 6, 64])\n",
      "First token’s embedding (partial): tensor([ 1.9269,  1.4873,  0.9007, -2.1055,  0.6784])\n"
     ]
    }
   ],
   "source": [
    "# Define sizes\n",
    "batch_size = 1    # One sentence\n",
    "seq_length = 6    # 6 tokens: 法國, 紅酒, 慢煮, 阿根廷, 牛舌, 配\n",
    "embed_dim = 64    # Embedding size per token\n",
    "\n",
    "# Simulate embeddings\n",
    "X = torch.randn(batch_size, seq_length, embed_dim)\n",
    "print(\"Input shape:\", X.shape)  # [1, 6, 64]\n",
    "print(\"First token’s embedding (partial):\", X[0, 0, :5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Build Multi-Head Attention\n",
    "\n",
    "Let’s create a `MultiHeadAttention` class. We’ll use 4 heads, each with a smaller dimension (e.g., 16 instead of 64), so the total size matches the input after combining."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads  # Each head’s size\n",
    "\n",
    "        # Ensure embed_dim is divisible by num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        # Linear layers for Q, K, V across all heads\n",
    "        self.query = nn.Linear(embed_dim, embed_dim)\n",
    "        self.key = nn.Linear(embed_dim, embed_dim)\n",
    "        self.value = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)  # Combine heads\n",
    "\n",
    "        self.scale = self.head_dim ** -0.5  # Scaling factor\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_length, embed_dim = x.size()\n",
    "\n",
    "        # Step 1: Compute Q, K, V\n",
    "        Q = self.query(x)  # [1, 6, 64]\n",
    "        K = self.key(x)    # [1, 6, 64]\n",
    "        V = self.value(x)  # [1, 6, 64]\n",
    "\n",
    "        # Step 2: Split into multiple heads\n",
    "        Q = Q.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # [1, 4, 6, 16]\n",
    "        K = K.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # [1, 4, 6, 16]\n",
    "        V = V.view(batch_size, seq_length, self.num_heads, self.head_dim).transpose(1, 2)  # [1, 4, 6, 16]\n",
    "\n",
    "        # Step 3: Compute attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale  # [1, 4, 6, 6]\n",
    "\n",
    "        # Step 4: Apply softmax for weights\n",
    "        attn_weights = torch.softmax(scores, dim=-1)  # [1, 4, 6, 6]\n",
    "\n",
    "        # Step 5: Combine with values\n",
    "        out = torch.matmul(attn_weights, V)  # [1, 4, 6, 16]\n",
    "\n",
    "        # Step 6: Reshape and combine heads\n",
    "        out = out.transpose(1, 2).contiguous().view(batch_size, seq_length, embed_dim)  # [1, 6, 64]\n",
    "        out = self.out(out)  # Final linear layer: [1, 6, 64]\n",
    "\n",
    "        return out, attn_weights\n",
    "\n",
    "# Test it\n",
    "num_heads = 4\n",
    "mha = MultiHeadAttention(embed_dim, num_heads)\n",
    "output, attn_weights = mha(X)\n",
    "print(\"Output shape:\", output.shape)  # [1, 6, 64]\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # [1, 4, 6, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Down the Code\n",
    "- **Sizes**: `embed_dim = 64`, `num_heads = 4`, so each head’s `head_dim = 16` (64 ÷ 4).\n",
    "- **Q, K, V**: Initially computed for all heads together (`[1, 6, 64]`), then split into 4 heads (`[1, 4, 6, 16]`).\n",
    "- **Scores**: Each head computes its own `[6, 6]` score matrix, so we get `[1, 4, 6, 6]`.\n",
    "- **Output**: After attention, heads are combined back to `[1, 6, 64]`.\n",
    "\n",
    "Each head might focus differently—e.g., Head 1 on cooking (`慢煮`), Head 2 on origin (`阿根廷`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Build a Transformer Block\n",
    "\n",
    "### What’s a Transformer Block?\n",
    "A transformer block is a complete unit in a transformer model. It has two main parts:\n",
    "1. **Multi-Head Attention**: Captures relationships between tokens (what we just built).\n",
    "2. **Feed-Forward Network (FFN)**: Processes each token individually to add deeper understanding.\n",
    "\n",
    "It also includes:\n",
    "- **Layer Normalization**: Stabilizes the numbers (like adjusting volume so everything’s clear).\n",
    "- **Residual Connections**: Adds the input back to the output (helps the model learn better).\n",
    "\n",
    "### Why These Parts?\n",
    "- **Attention**: “Hey, `牛舌`, look at `慢煮`!”\n",
    "- **FFN**: “Let’s think more about what `牛舌` means now.”\n",
    "- **Norm & Residual**: Keeps everything balanced and prevents forgetting the original input.\n",
    "\n",
    "Let’s code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Multi-head attention\n",
    "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
    "        # Feed-forward network\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(ff_hidden_dim, embed_dim)\n",
    "        )\n",
    "        # Layer normalization\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "        # Dropout to prevent overfitting\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head attention with residual connection\n",
    "        attn_output, attn_weights = self.attention(x)\n",
    "        x = self.norm1(x + self.dropout(attn_output))  # Add & norm\n",
    "\n",
    "        # Feed-forward with residual connection\n",
    "        ff_output = self.ffn(x)\n",
    "        x = self.norm2(x + self.dropout(ff_output))  # Add & norm\n",
    "\n",
    "        return x, attn_weights\n",
    "\n",
    "# Test it\n",
    "ff_hidden_dim = 256  # Hidden layer size in FFN\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_hidden_dim)\n",
    "output, attn_weights = transformer_block(X)\n",
    "print(\"Transformer output shape:\", output.shape)  # [1, 6, 64]\n",
    "print(\"Attention weights shape:\", attn_weights.shape)  # [1, 4, 6, 6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Breaking Down the Transformer Block\n",
    "- **Attention**: Computes relationships, outputs `[1, 6, 64]` and weights `[1, 4, 6, 6]`.\n",
    "- **Residual**: `x + attn_output` keeps the original input’s info.\n",
    "- **Norm1**: Normalizes after attention.\n",
    "- **FFN**: Expands to 256 dims, applies ReLU, then shrinks back to 64.\n",
    "- **Norm2**: Normalizes after FFN.\n",
    "- **Dropout**: Randomly drops some values (10% here) to avoid over-reliance.\n",
    "\n",
    "This block refines `牛舌` by first connecting it to other tokens, then thinking deeper about its meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Multi-Head Attention Weights\n",
    "\n",
    "Let’s plot the attention weights from all 4 heads to see their different focuses. We’ll use a heatmap for each head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set font for Chinese characters\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']  # Use SimHei for Chinese\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# Tokens for labeling\n",
    "tokens = ['法國', '紅酒', '慢煮', '阿根廷', '牛舌', '配']\n",
    "\n",
    "# Convert weights to numpy\n",
    "attn_weights_np = attn_weights[0].detach().numpy()  # [4, 6, 6]\n",
    "\n",
    "# Plot heatmaps for all heads\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(num_heads):\n",
    "    sns.heatmap(attn_weights_np[i], annot=True, cmap='Blues', fmt='.2f',\n",
    "                xticklabels=tokens, yticklabels=tokens, ax=axes[i])\n",
    "    axes[i].set_title(f'Head {i+1} 注意力權重 (Attention Weights)')\n",
    "    axes[i].set_xlabel('被關注的詞 (Keys)')\n",
    "    axes[i].set_ylabel('關注的詞 (Queries)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"注意: 如果中文未顯示，請確保系統有 SimHei 字體。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What Do We See?\n",
    "- **Head 1**: Might focus on cooking (`牛舌` → `慢煮`, `紅酒`).\n",
    "- **Head 2**: Might focus on origin (`牛舌` → `阿根廷`).\n",
    "- **Head 3**: Might focus on pairing (`牛舌` → `配`).\n",
    "- **Head 4**: Could catch other patterns (e.g., `法國` → `紅酒`).\n",
    "\n",
    "Each head’s heatmap (6x6) shows how every token attends to every other token. High values (dark blue) mean strong focus. This diversity is why multi-head attention is powerful!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Exercises for Practice\n",
    "\n",
    "Try these to solidify your understanding:\n",
    "1. **Change Number of Heads**: Set `num_heads` to 2 or 8. Re-run and check the output shape and heatmaps. How does the number of heads affect patterns?\n",
    "2. **Adjust FFN Size**: Change `ff_hidden_dim` to 128 or 512. Does the output shape change? Why or why not?\n",
    "3. **Visualize One Token**: Print `attn_weights[0, :, 4, :]` (weights for `牛舌`) for all heads. Which head focuses most on `慢煮`?\n",
    "4. **Add Tokens**: Increase `seq_length` to 8 (add `煙肉`, `洋蔥`). Re-run and observe the heatmaps.\n",
    "\n",
    "Write your code below and experiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your exercise code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You’ve conquered a lot today! Here’s what we did:\n",
    "- **Multi-Head Attention**: Built a system where multiple heads (e.g., 4) catch different relationships in our sentence—like cooking, origin, and pairing for `牛舌`.\n",
    "- **Transformer Block**: Added a feed-forward network, normalization, and residuals to refine token meanings.\n",
    "- **Visualization**: Plotted attention weights to see each head’s unique focus.\n",
    "\n",
    "Next up in Lesson 5, we’ll stack multiple transformer blocks to build a mini transformer model and generate text. You’re getting closer to understanding LLMs like the pros!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
