{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Lesson 5: Building the Language Model\n",
          "\n",
          "Welcome to Lesson 5! In this lesson, we’re going to put together everything you’ve learned so far to build a complete language model. By the end of this lesson, you’ll understand how to assemble multiple transformer blocks into a functioning model that can process and generate text.\n",
          "\n",
          "## Recap of Previous Lessons\n",
          "- **Lesson 1**: Introduction to LLMs and tokenization.\n",
          "- **Lesson 2**: Embeddings and how they represent tokens.\n",
          "- **Lesson 3**: Single-head attention mechanism.\n",
          "- **Lesson 4**: Multi-head attention and transformer blocks.\n",
          "\n",
          "Now, we’ll take those transformer blocks and stack them to create a language model capable of understanding and generating text, using our familiar Chinese example: *法國紅酒慢煮阿根廷牛舌 配 煙肉洋蔥炒著仔*.\n",
          "\n",
          "## What You’ll Learn\n",
          "- Why we need positional encoding and how it works.\n",
          "- How to stack multiple transformer blocks to build a deeper model.\n",
          "- How to add an output layer for generating predictions.\n",
          "- The basics of training a language model.\n",
          "- A practical example using mock data inspired by our Chinese text.\n",
          "\n",
          "Let’s dive in!"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": 1,
        "metadata": {},
        "outputs": [],
        "source": [
          "import torch\n",
          "import torch.nn as nn\n",
          "import pandas as pd\n",
          "import matplotlib.pyplot as plt\n",
          "import numpy as np\n",
          "\n",
          "# Set random seed for reproducibility\n",
          "torch.manual_seed(42)"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Understanding Positional Encoding\n",
          "\n",
          "### Why Do We Need Positional Encoding?\n",
          "Imagine you’re reading *法國紅酒慢煮阿根廷牛舌*. The order of words matters: “France red wine slow-cook Argentina beef tongue” makes sense, but “beef tongue Argentina slow-cook red wine France” is confusing! Transformers, unlike older models like RNNs, process all tokens at once (in parallel), not sequentially. This speed is great, but it means they don’t naturally understand word order. **Positional encoding** fixes this by adding information about each token’s position in the sequence.\n",
          "\n",
          "### How Does It Work?\n",
          "Positional encoding adds a unique “fingerprint” to each token’s embedding based on its position (e.g., 1st, 2nd, 3rd). In our code, we use a mathematical trick with **sine and cosine functions**:\n",
          "- For each position (e.g., 0 for `法國`, 1 for `紅酒`), we create a vector of the same size as the token embedding (e.g., 64 numbers).\n",
          "- The vector’s values alternate between sine and cosine, with frequencies that change based on the position and dimension. This creates a unique pattern for every spot in the sequence.\n",
          "- We add this positional vector to the token’s embedding, so the model knows both *what* the token is (from the embedding) and *where* it is (from the positional encoding).\n",
          "\n",
          "### Example\n",
          "- Token `法國` at position 0 might get embedding `[0.1, -0.2, 0.5, ...]`.\n",
          "- Positional encoding for position 0 could be `[0.0, 1.0, 0.0, ...]` (simplified).\n",
          "- Combined: `[0.1, 0.8, 0.5, ...]`—now it’s marked as the first word.\n",
          "- `紅酒` at position 1 gets a different positional vector, like `[0.84, 0.54, ...]`, shifting its embedding.\n",
          "\n",
          "### Why Sine and Cosine?\n",
          "- These functions create smooth, repeating patterns that the model can learn.\n",
          "- They work for any sequence length (up to a max) because the pattern scales predictably.\n",
          "- The alternating frequencies (high for early dimensions, low for later ones) let the model distinguish nearby vs. far-away positions.\n",
          "\n",
          "In short, positional encoding is like giving each word a seat number at a table—it helps the transformer know who’s sitting where!"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": 2,
        "metadata": {},
        "outputs": [],
        "source": [
          "class PositionalEncoding(nn.Module):\n",
          "    def __init__(self, embed_dim, max_seq_length):\n",
          "        super().__init__()\n",
          "        # Create a fixed positional encoding matrix\n",
          "        pe = torch.zeros(max_seq_length, embed_dim)\n",
          "        position = torch.arange(0, max_seq_length).unsqueeze(1)  # [max_seq_length, 1]\n",
          "        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(np.log(10000.0) / embed_dim))\n",
          "        pe[:, 0::2] = torch.sin(position * div_term)  # Even indices\n",
          "        pe[:, 1::2] = torch.cos(position * div_term)  # Odd indices\n",
          "        self.register_buffer('pe', pe)  # Store it as a buffer (not trainable)\n",
          "\n",
          "    def forward(self, x):\n",
          "        # Add positional encoding to the input embeddings\n",
          "        seq_len = x.size(1)\n",
          "        x = x + self.pe[:seq_len, :]\n",
          "        return x"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": 3,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Transformer components from Lesson 4, simplified with PyTorch's built-in attention\n",
          "class MultiHeadAttention(nn.Module):\n",
          "    def __init__(self, embed_dim, num_heads):\n",
          "        super().__init__()\n",
          "        self.attention = nn.MultiheadAttention(embed_dim, num_heads)\n",
          "\n",
          "    def forward(self, x):\n",
          "        x = x.transpose(0, 1)  # [seq_len, batch_size, embed_dim]\n",
          "        attn_output, attn_weights = self.attention(x, x, x)\n",
          "        attn_output = attn_output.transpose(0, 1)  # [batch_size, seq_len, embed_dim]\n",
          "        return attn_output, attn_weights\n",
          "\n",
          "class TransformerBlock(nn.Module):\n",
          "    def __init__(self, embed_dim, num_heads, ff_hidden_dim, dropout=0.1):\n",
          "        super().__init__()\n",
          "        self.attention = MultiHeadAttention(embed_dim, num_heads)\n",
          "        self.ffn = nn.Sequential(\n",
          "            nn.Linear(embed_dim, ff_hidden_dim),\n",
          "            nn.ReLU(),\n",
          "            nn.Linear(ff_hidden_dim, embed_dim)\n",
          "        )\n",
          "        self.norm1 = nn.LayerNorm(embed_dim)\n",
          "        self.norm2 = nn.LayerNorm(embed_dim)\n",
          "        self.dropout = nn.Dropout(dropout)\n",
          "\n",
          "    def forward(self, x):\n",
          "        attn_output, attn_weights = self.attention(x)\n",
          "        x = self.norm1(x + self.dropout(attn_output))\n",
          "        ff_output = self.ffn(x)\n",
          "        x = self.norm2(x + self.dropout(ff_output))\n",
          "        return x, attn_weights"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": 4,
        "metadata": {},
        "outputs": [],
        "source": [
          "class LanguageModel(nn.Module):\n",
          "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, ff_hidden_dim, max_seq_length, dropout=0.1):\n",
          "        super().__init__()\n",
          "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
          "        self.positional_encoding = PositionalEncoding(embed_dim, max_seq_length)\n",
          "        self.transformer_blocks = nn.ModuleList([\n",
          "            TransformerBlock(embed_dim, num_heads, ff_hidden_dim, dropout)\n",
          "            for _ in range(num_layers)\n",
          "        ])\n",
          "        self.output_layer = nn.Linear(embed_dim, vocab_size)\n",
          "\n",
          "    def forward(self, x):\n",
          "        x = self.embedding(x)  # [batch_size, seq_length] -> [batch_size, seq_length, embed_dim]\n",
          "        x = self.positional_encoding(x)\n",
          "        for block in self.transformer_blocks:\n",
          "            x, _ = block(x)  # Pass through each transformer block\n",
          "        logits = self.output_layer(x)  # [batch_size, seq_length, vocab_size]\n",
          "        return logits"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Is This Like LLaMA 2, Just Smaller?\n",
          "\n",
          "You might wonder if this model is a mini version of LLaMA 2. The short answer is **yes, but with simplifications**. Let’s compare:\n",
          "\n",
          "### Similarities to LLaMA 2\n",
          "- **Transformer-Based**: Both use stacked transformer blocks with multi-head attention, feed-forward networks, and layer normalization.\n",
          "- **Positional Encoding**: LLaMA 2 uses a form of positional encoding (though it’s a variant called RoPE—Rotary Position Embedding), while we use the classic sine/cosine method.\n",
          "- **Decoder-Only**: Our model is a decoder-only architecture (like LLaMA 2), meaning it’s designed to predict the next token based on previous ones, perfect for generating text.\n",
          "- **Scalability**: The structure (embedding → positional encoding → transformer blocks → output) mirrors LLaMA 2’s core design, just with fewer layers and smaller sizes.\n",
          "\n",
          "### Differences from LLaMA 2\n",
          "- **Size**: LLaMA 2 has models ranging from 7B to 70B parameters (e.g., billions of weights), with `embed_dim` around 4096, 32+ layers, and 32+ heads. Ours is tiny—e.g., `embed_dim=64`, 2 layers, 4 heads, with maybe a few million parameters at most.\n",
          "- **Positional Encoding**: LLaMA 2 uses RoPE, which rotates embeddings based on position (more efficient for long sequences), while we use the original Transformer’s sine/cosine method.\n",
          "- **Optimization**: LLaMA 2 includes advanced tricks like grouped-query attention (fewer keys/values per head) and RMSNorm (a variant of LayerNorm). We stick to basic multi-head attention and standard LayerNorm for simplicity.\n",
          "- **Training Data**: LLaMA 2 was trained on massive, diverse datasets (trillions of tokens). Ours is a toy model with mock data.\n",
          "- **Efficiency**: LLaMA 2 uses techniques like FlashAttention and mixed precision for speed. We use PyTorch’s built-in attention, which is slower but easier to understand.\n",
          "\n",
          "### Verdict\n",
          "This is a **scaled-down, simplified cousin** of LLaMA 2. It captures the core ideas—transformer blocks, attention, and next-token prediction—but skips the advanced optimizations and massive scale that make LLaMA 2 a state-of-the-art model. Think of ours as a learning tool: it’s the same family, just small enough to fit in your hands!"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Input and Output\n",
          "\n",
          "### Input\n",
          "- A batch of token indices (e.g., `[0, 1, 2]` for `法國`, `紅酒`, `慢煮`).\n",
          "- Shape: `[batch_size, seq_length]`.\n",
          "\n",
          "### Processing\n",
          "- **Embedding**: Turns indices into vectors (e.g., `[batch_size, seq_length, embed_dim]`).\n",
          "- **Positional Encoding**: Adds position info.\n",
          "- **Transformer Blocks**: Update token representations with attention and feed-forward layers.\n",
          "- **Output Layer**: Produces logits for each token’s next prediction.\n",
          "\n",
          "### Output\n",
          "- Logits: Scores for each possible next token (e.g., `[batch_size, seq_length, vocab_size]`).\n",
          "- For training, we predict the next token: input `法國紅酒慢煮` → target `紅酒慢煮阿根廷`."
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Training the Language Model\n",
          "\n",
          "Training teaches the model to predict the next token accurately. Here’s how:\n",
          "\n",
          "- **Loss Function**: Cross-entropy loss compares the model’s predictions (logits) to the true next tokens.\n",
          "- **Optimizer**: Adam adjusts the model’s weights to reduce the loss.\n",
          "- **Training Loop**: Repeatedly process batches of data, calculate loss, and update weights.\n",
          "\n",
          "Sample training loop (conceptual):\n",
          "```python\n",
          "model = LanguageModel(...)\n",
          "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
          "criterion = nn.CrossEntropyLoss()\n",
          "for epoch in range(num_epochs):\n",
          "    for inputs, targets in data_loader:\n",
          "        optimizer.zero_grad()\n",
          "        logits = model(inputs)\n",
          "        loss = criterion(logits.view(-1, vocab_size), targets.view(-1))\n",
          "        loss.backward()\n",
          "        optimizer.step()\n",
          "```\n",
          "\n",
          "This loop refines the model over time, making it better at guessing what comes next!"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": 5,
        "metadata": {},
        "outputs": [
          {
            "name": "stdout",
            "output_type": "stream",
            "text": [
              "Logits shape: torch.Size([2, 10, 1000])\n"
            ]
          }
        ],
        "source": [
          "# Practical Example with Mock Data\n",
          "\n",
          "# Step 1: Mock Data (simulating tokenized Chinese text)\n",
          "vocab_size = 1000  # Imagine 1000 unique tokens like 法國, 紅酒, etc.\n",
          "seq_length = 10    # Short sequence length\n",
          "batch_size = 2     # Two example sentences\n",
          "\n",
          "# Random token indices (pretend these are from our sentence)\n",
          "inputs = torch.randint(0, vocab_size, (batch_size, seq_length))\n",
          "targets = torch.randint(0, vocab_size, (batch_size, seq_length))  # Shifted for next-token prediction\n",
          "\n",
          "# Step 2: Define the Model\n",
          "embed_dim = 64\n",
          "num_heads = 4\n",
          "num_layers = 2\n",
          "ff_hidden_dim = 256\n",
          "max_seq_length = 50\n",
          "\n",
          "model = LanguageModel(vocab_size, embed_dim, num_heads, num_layers, ff_hidden_dim, max_seq_length)\n",
          "\n",
          "# Step 3: Forward Pass\n",
          "logits = model(inputs)\n",
          "print(\"Logits shape:\", logits.shape)  # [2, 10, 1000]\n",
          "\n",
          "# Note: In a real setup, you’d use a tokenizer on 'data.csv' and train with actual data."
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Exercises\n",
          "\n",
          "1. **Change Layers**: Set `num_layers` to 1 or 4. Does the output shape change? Why or why not?\n",
          "2. **Adjust Embedding Size**: Try `embed_dim = 32` or `128`. Run the model and check the logits shape.\n",
          "3. **Visualize Positional Encoding**: Create a small `PositionalEncoding` instance (e.g., `embed_dim=4`, `max_seq_length=5`) and print `pe` to see the patterns.\n",
          "4. **Attention Weights**: Modify `LanguageModel.forward` to return attention weights from the last block. Plot them for one input sequence.\n",
          "\n",
          "Experiment below!"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "# Your exercise code here\n",
          "\n"
        ]
      },
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "## Summary\n",
          "\n",
          "Great job! You’ve built a language model from scratch. Here’s what we covered:\n",
          "- **Positional Encoding**: Adds order to tokens using sine/cosine patterns.\n",
          "- **Stacking Blocks**: Combined transformer blocks for deeper understanding.\n",
          "- **Output Layer**: Predicts the next token with logits.\n",
          "- **Training Basics**: Learned how loss and optimization work.\n",
          "- **Comparison to LLaMA 2**: Saw how our model is a simplified version of advanced LLMs.\n",
          "\n",
          "Next, in Lesson 6, we’ll fine-tune this model and generate text with it. You’re almost ready to create your own mini-LLaMA!"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": ".venv",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.11.11"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 4
  }